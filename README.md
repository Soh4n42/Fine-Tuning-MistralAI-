# ğŸŸï¸ Sports Headline Generator using Mistral-7B + LoRA

This project fine-tunes [Mistral-7B-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3) using Low-Rank Adaptation (LoRA) to generate **catchy and realistic sports news headlines** from a simple instruction prompt.

---

## ğŸ“¦ Project Structure

â”œâ”€â”€ data_collection/ # Scripts or notebooks for scraping and preparing sports headlines

    â”œâ”€â”€scrape.py
    â”œâ”€â”€prompt_formatting.py
    â”œâ”€â”€train.txt
    â”œâ”€â”€train.json
    
â”œâ”€â”€ ft_mistral.ipynb # Notebook to fine-tune Mistral-7B using PEFT + TRL

â”œâ”€â”€ README.md # Project documentation

â”œâ”€â”€ pyproject.toml # Dependency and configuration management

â”œâ”€â”€ .gitignore # Files to ignore in version control



---

## ğŸ“Š Dataset

You create a custom dataset of sports news headlines extracted from sources like:

- Espn
- Fox Sports
- BBC
- CNN

### ğŸ”§ Format: `train.json`

```json
{"prompt": "Generate a sports news headline.", "response": "Spain begin Euro 2025 campaign by thrashing Portugal"}
{"prompt": "Generate a sports news headline.", "response": "Can Raducanu bridge gap to world's best Sabalenka?"}
```


ğŸ§  Model & Training
âœ… Base model: mistralai/Mistral-7B-Instruct-v0.1

âœ… Quantization: 4-bit using bitsandbytes

âœ… PEFT: LoRA for memory-efficient fine-tuning

âœ… Frameworks: transformers, trl, peft, datasets

ğŸ‹ï¸ Training Objective
To teach the model to generate well-formed, realistic headlines from a fixed instruction prompt.


### ğŸ” Inference Example

```python
from transformers import pipeline

generator = pipeline("text-generation", model="your-model", tokenizer="your-tokenizer")

prompt = "Generate a sports news headline."
output = generator(prompt, max_new_tokens=30)
print(output[0]["generated_text"])

